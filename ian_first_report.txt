Our use of copy optimizations was the most successful approach that we found for increasing performance. The issue in the naive implementation was that for the innermost loop of the calculations, matrix B was accessed with unit stride but matrix A was accessed with a stride of M. This non-unit stride vastly reduced how effectively we could cache matrix A in a useful way. To fix this we simply transposed matrix A to a new array in row-major order. This allowed us to access both matrices, A and B, in unit stride in the innermost loop. This alone boosted our performance above all other implementations except for MKL and OpenBLAS.

After the copy optimizations, we tried to include explicit SSE instructions to operate on multiple doubles at once. This, however, was unsuccessful. We suspect the intel compiler already did a good job of leveraging vector instructions since the performance only decreased when we tried to use methods from xmmintrin.h. Although we did end up getting this implementation to work, the added overhead and complications of using data types and operations from xmmintrin.h only resulted in slowing down our implementation.

We also encountered issues when trying to merge differing strategies together. Our attempts at modifying the loop order of j, k, and i didn't mesh with the copy optimizations at all since the order of the loops dictated the access patterns. For now we prioritized the copy optimizations because it was far much more successful at increasing performance. We also had to modify where we did the matrix transpose when we combined with blocking. Since we changed one matrix to be row-major order, we had to modify how parameters were being passed between the functions for blocking.